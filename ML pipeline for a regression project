#Installing Required Libraries
!pip install pyspark==3.1.2 -q
!pip install findspark -q

#Importing Required Libraries
#suppress warnings generated by your code
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

#ETL
#Import required libraries
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import StandardScaler

# Create a spark session
spark = SparkSession.builder.appName("Practice Project").getOrCreate()

#Load the csv file into a dataframe
!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv

#Load the dataset into the spark dataframe
df = spark.read.csv("mpg-raw.csv", header=True, inferSchema=True)

#Print the number of cars in each Origin
df.groupBy('Origin').count().orderBy('count').show()

#Drop all the duplicate rows from the dataset
df = df.dropDuplicates()

#Drop all the rows that contain null values from the dataset
df=df.dropna()

#Rename the column "Engine Disp" to "Engine_Disp"Dro
df = df.withColumnRenamed("Engine Disp","Engine_Disp")

#Save the dataframe in parquet format, name the file as "mpg-cleaned.parquet"
df.write.mode("overwrite").parquet("mpg-cleaned.parquet")

#Evaluation
print("Part 1 - Evaluation")

print("Total rows = ", rowcount1)
print("Total rows after dropping duplicate rows = ", rowcount2)
print("Total rows after dropping duplicate rows and rows with null values = ", rowcount3)
print("Renamed column name = ", df.columns[2])

import os

print("mpg-cleaned.parquet exists :", os.path.isdir("mpg-cleaned.parquet"))

#Machine Learning Pipeline creation
#Define the StringIndexer pipeline stage
indexer = StringIndexer(inputCol="Origin", outputCol="OriginIndex")

#Define the VectorAssembler pipeline stage
assembler = VectorAssembler(inputCols=['Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year'], outputCol="features")

#Define the StandardScaler pipeline stage
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

#Define the Model creation pipeline stage
lr = LinearRegression(featuresCol="scaledFeatures", labelCol="MPG")

#Build the pipeline
pipeline = Pipeline(stages=[indexer,assembler, scaler, lr])

#Split the data
(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)

#Fit the pipeline
pipelineModel = pipeline.fit(trainingData)

#Evaluation
print("Part 2 - Evaluation")
print("Total rows = ", rowcount4)
ps = [str(x).split("_")[0] for x in pipeline.getStages()]

print("Pipeline Stage 1 = ", ps[0])
print("Pipeline Stage 2 = ", ps[1])
print("Pipeline Stage 3 = ", ps[2])

print("Label column = ", lr.getLabelCol())
